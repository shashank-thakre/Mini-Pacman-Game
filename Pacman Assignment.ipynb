{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mini_pacman import test, random_strategy, naive_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mini_pacman import PacmanGame\n",
    "\n",
    "with open('test_params.json', 'r') as file:\n",
    "    read_params = json.load(file)\n",
    "game_params = read_params['params']\n",
    "env = PacmanGame(**game_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': 0,\n",
       " 'total_score': 0,\n",
       " 'end_game': False,\n",
       " 'player': (2, 6),\n",
       " 'monsters': [(0, 7), (7, 7)],\n",
       " 'diamonds': [(2, 5), (4, 3), (6, 1)],\n",
       " 'walls': [(2, 0),\n",
       "  (2, 3),\n",
       "  (3, 1),\n",
       "  (3, 4),\n",
       "  (3, 7),\n",
       "  (4, 2),\n",
       "  (4, 5),\n",
       "  (5, 6),\n",
       "  (6, 0),\n",
       "  (6, 5)],\n",
       " 'possible_actions': [1, 2, 4, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.get_obs()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    v = []\n",
    "    x,y = obs['player']\n",
    "    v.append(x)\n",
    "    v.append(y)\n",
    "    for x, y in obs['monsters']:\n",
    "        v.append(x)\n",
    "        v.append(y)\n",
    "    for x, y in obs['diamonds']:\n",
    "        v.append(x)\n",
    "        v.append(y)\n",
    "    for x, y in obs['walls']:\n",
    "        v.append(x)\n",
    "        v.append(y)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32, input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(Dense(units=72, activation='relu'))\n",
    "    model.add(Dense(units=36, activation='relu'))\n",
    "    model.add(Dense(nb_actions, activation='relu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (32,)\n",
      "nb_actions:  9\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32,)\n",
    "nb_actions = 9\n",
    "print('input_shape: ',input_shape)\n",
    "print('nb_actions: ',nb_actions)\n",
    "\n",
    "online_network = create_dqn_model(input_shape, nb_actions)\n",
    "online_network.compile(optimizer=Adam(0.001), loss='mse') #0.001 is the learning rate\n",
    "target_network = clone_model(online_network)\n",
    "target_network.set_weights(online_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online_network = load_model('output/sv_pacman_dbl_dqn_model.h5')\n",
    "# target_network = load_model('output/sv_pacman_dbl_dqn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon, n_outputs):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(n_outputs)  # random action\n",
    "    else:\n",
    "        return np.argmax(q_values)+1  # q-optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 300000 # number of times \n",
    "warmup = 10000 # first iterations after random initiation before training starts\n",
    "training_interval = 4 # number of steps after which dqn is retrained\n",
    "copy_steps = 10_000 # number of steps after which weights of \n",
    "                   # online network copied into target network\n",
    "gamma = 0.999999 # discount rate\n",
    "batch_size = 64 # size of batch from replay memory \n",
    "eps_max = 1.0 # parameters of decaying sequence of eps\n",
    "eps_min = 0.05\n",
    "eps_decay_steps = 75_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "replay_memory_maxlen = 1_000_000\n",
    "replay_memory = deque([], maxlen=replay_memory_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "iteration = 0\n",
    "done = True\n",
    "\n",
    "\n",
    "while step < n_steps:\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        old_state = get_state(obs)\n",
    "    iteration += 1\n",
    "    q_values = online_network.predict(np.array([old_state]))[0]  \n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    action = epsilon_greedy(q_values, epsilon, obs['possible_actions'])\n",
    "    next_obs = env.make_action(action)\n",
    "    new_state = get_state(next_obs)\n",
    "    reward = next_obs['reward']\n",
    "    done = next_obs[\"end_game\"]\n",
    "    replay_memory.append((old_state, action, reward, new_state, done))\n",
    "    old_state = new_state\n",
    "\n",
    "    if iteration >= warmup and iteration % training_interval == 0:\n",
    "        step += 1\n",
    "        minibatch = random.sample(replay_memory, batch_size)\n",
    "        replay_state = np.array([x[0] for x in minibatch])\n",
    "        replay_action = np.array([x[1] for x in minibatch])\n",
    "        replay_rewards = np.array([x[2] for x in minibatch])\n",
    "        replay_next_state = np.array([x[3] for x in minibatch])\n",
    "        replay_done = np.array([x[4] for x in minibatch], dtype=int)\n",
    "        #target_for_action = replay_rewards + (1-replay_done) * gamma * np.amax(target_network.predict(replay_next_state), axis=1)\n",
    "        # Double DQN Method below\n",
    "        best_actions = np.argmax(online_network.predict(replay_next_state), axis=1)\n",
    "        target_for_action = replay_rewards + (1-replay_done) * gamma * \\\n",
    "                                    target_network.predict(replay_next_state)[np.arange(batch_size), best_actions]\n",
    "        # Double DQN Method above\n",
    "        target = online_network.predict(replay_state)  # targets coincide with predictions ...\n",
    "        replay_action = replay_action - 1\n",
    "        target[np.arange(batch_size), replay_action] = target_for_action  #...except for targets with actions from replay\n",
    "        online_network.fit(replay_state, target, epochs=step, verbose=0, initial_epoch=step-1)\n",
    "        if step % copy_steps == 0:\n",
    "            target_network.set_weights(online_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_network.save('sv_pacman_dbl_dqn_model_300l_lptp.h5')\n",
    "pacman_dqn_model = load_model('sv_pacman_dbl_dqn_model_300l_lptp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pacman_dqn_model = load_model('output/sv_pacman_dbl_dqn_model.h5')\n",
    "def dqn_strategy(obs):\n",
    "    new_state = get_state(obs)\n",
    "    q_values = pacman_dqn_model.predict(np.array([new_state]))[0]  \n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    action = epsilon_greedy(q_values, epsilon, obs['possible_actions'])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average score is 284.218, median is 205.5, saved log to 'test_pacman_log_dbl_DQN1.json'. Do not forget to upload it for submission!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "205.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(strategy=dqn_strategy, log_file='test_pacman_log_dbl_DQN1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacman_dqn_model = load_model('sv_pacman_dbl_dqn_model_300l_lptp.h5')\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "state = get_state(obs)\n",
    "step = 0\n",
    "iteration = 0\n",
    "#done = True\n",
    "while not obs['end_game']:\n",
    "    time.sleep(0.1)\n",
    "    # select best next action using Q-Learning (no random component here, eps=0)\n",
    "    action = dqn_strategy(obs)\n",
    "    obs = env.make_action(action)\n",
    "    state = get_state(obs)\n",
    "    env.render()\n",
    "\n",
    "print('Total score = {}'.format(obs['total_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
